# N8N Voice Agent Workflow Optimization Guide

## Executive Summary
Target: Reduce response latency from ~2-3s to <800ms for real-time voice

## Critical Changes (Immediate Impact)

### 1. REMOVE MCP LOOP âš¡ (-300ms)
**Current Architecture (SLOW):**
```
Webhook â†’ Agent â†’ MCP Client (HTTP) â†’ MCP Server Trigger â†’ Tools
```

**Optimized Architecture:**
```
Webhook â†’ Agent â†’ Tools (Direct Connection)
```

**Action Steps:**
1. Delete `MCP Server Trigger1` node
2. Delete `MCP Client1` node
3. Connect tools DIRECTLY to `Conversation Agent1`:
   - `LookupByWaybill1` â†’ Agent (ai_tool input)
   - `LookupByPhone1` â†’ Agent (ai_tool input)
   - `Knowledge Base1` â†’ Agent (ai_tool input)

**Why This Works:**
- Eliminates HTTP roundtrip overhead
- No serialization/deserialization delay
- Direct in-process tool execution

---

### 2. KNOWLEDGE BASE OPTIMIZATION âš¡ (-100ms)

**Current Setting:**
```json
"topK": 3  // Retrieving 3 docs but using only 1
```

**Optimized:**
```json
"topK": 1,
"includeDocumentMetadata": false  // âœ… Already set
```

**Additional Settings:**
```json
"options": {
  "scoreThreshold": 0.7  // Skip irrelevant results
}
```

---

### 3. MODEL OPTIMIZATION âš¡ (-200ms)

**Current:**
```json
"model": "gpt-4.1-mini",
"maxTokens": 200,
"temperature": 0,
"topP": 0
```

**Optimized for Voice:**
```json
"model": "gpt-4o-mini-2024-07-18",  // Faster inference
"maxTokens": 150,  // Sufficient for stage-based responses
"temperature": 0.3,  // Slight variance for natural speech
"topP": 1,  // Better with low temp
"frequencyPenalty": 0.3,  // Reduce repetition in voice
"presencePenalty": 0.1
```

**Alternative (Fastest):**
```json
"model": "gpt-3.5-turbo",  // 40% faster, still capable for structured scripts
"maxTokens": 120
```

---

### 4. MEMORY OPTIMIZATION âš¡ (-50ms)

**Current:**
- Every request hits Postgres
- Full history loaded each time

**Optimized Settings:**
```json
"parameters": {
  "sessionIdType": "customKey",
  "sessionKey": "={{ $json.body.session_id }}",
  "tableName": "n8n_chat_histories_test",
  "options": {
    "contextWindowLength": 10  // âœ… ADD THIS - Last 10 messages only
  }
}
```

**Advanced (For High Volume):**
Add Redis caching layer:
```
Webhook â†’ Check Redis â†’ [Cache Hit: Use cached context]
                      â†’ [Cache Miss: Postgres â†’ Update Redis]
```

---

### 5. STREAMING OPTIMIZATION âš¡ (-0ms latency, +perceived speed)

**Current Settings (Good):**
```json
"batching": {
  "batchSize": 1,
  "delayBetweenBatches": 0
}
```

**Add Webhook Response Headers (Already Set âœ…):**
```json
"Transfer-Encoding": "chunked",
"Cache-Control": "no-cache",
"X-Accel-Buffering": "no"
```

**NEW: Add to Agent Options:**
```json
"options": {
  "enableStreaming": true,  // âœ… Already enabled
  "streamTokens": true,  // âœ… ADD - Stream at token level
  "minTokensBeforeSend": 5  // âœ… ADD - Send every 5 tokens
}
```

---

## Advanced Optimizations

### 6. PARALLEL TOOL CALLS (When Applicable)

If agent needs multiple lookups, enable parallel execution:

**Update System Prompt - Add this section:**
```
## PERFORMANCE RULES
- When both waybill AND phone available â†’ call BOTH tools in parallel
- Use streaming responses - start speaking while tools execute
```

---

### 7. PROMPT CACHING âœ… Already Implemented

**Current (Good):**
```json
"promptCacheKey": "agent-naqel-v0.0"
```

**Ensure Version Control:**
- Increment version when system prompt changes
- Cache saves ~400ms on prompt processing

---

### 8. DATABASE QUERY OPTIMIZATION

**Data Table Tools Configuration:**
```json
"parameters": {
  "limit": 1,  // âœ… Good
  "outputMode": "firstItem"  // âœ… ADD - Skip array wrapping
}
```

**Backend Optimization (If you control n8n DB):**
```sql
-- Add indexes for faster lookups
CREATE INDEX idx_waybillNumber ON data_table(waybillNumber);
CREATE INDEX idx_senderPhone ON data_table(senderPhone);
```

---

## Alternative Architecture: Django Direct Integration

**Based on your selection, bypass n8n for tools:**

### Option A: Hybrid (Recommended)
```python
# Django Voice Consumer
class VoiceConsumer:
    def __init__(self):
        self.db_tools = NaqelDBTools()  # Local DB access
        self.kb_client = QdrantClient()  # Direct Qdrant
        self.n8n_agent = N8NAgentClient()  # Only for orchestration

    async def process(self, text: str, session_id: str):
        # 1. Fast local tool calls (0 network latency)
        waybill_data = await self.db_tools.lookup_waybill(text)
        kb_result = await self.kb_client.search(query, limit=1)

        # 2. Pass pre-fetched data to n8n agent
        response = await self.n8n_agent.generate(
            text=text,
            context={
                "waybill": waybill_data,
                "script": kb_result
            }
        )
        return response
```

**Latency Comparison:**
- Current: `800ms (tools) + 1200ms (LLM) = 2000ms`
- Optimized: `100ms (tools) + 1200ms (LLM) = 1300ms`
- Django Direct: `20ms (tools) + 1200ms (LLM) = 1220ms`

### Option B: Full Django (Maximum Performance)
```python
from langchain.agents import AgentExecutor
from langchain_openai import ChatOpenAI

class DirectVoiceAgent:
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o-mini", streaming=True)
        self.tools = [
            NaqelDBLookupTool(),  # Direct DB
            QdrantKBTool(),       # Direct Qdrant
        ]
        self.agent = AgentExecutor.from_tools(
            tools=self.tools,
            llm=self.llm,
            memory=PostgresChatMemory()
        )

    async def stream_response(self, text: str, session_id: str):
        async for chunk in self.agent.astream({"input": text}):
            yield chunk
```

**When to Use:**
- High volume (>1000 concurrent calls)
- Need <500ms total latency
- Complex tool orchestration

---

## Implementation Priority

### Phase 1: Quick Wins (1 hour) â†’ -400ms
1. âœ… Remove MCP loop
2. âœ… Set topK=1
3. âœ… Add contextWindowLength=10

### Phase 2: Model Tuning (30 min) â†’ -200ms
4. âœ… Switch to gpt-4o-mini
5. âœ… Adjust maxTokens to 150

### Phase 3: Advanced (If still needed) â†’ -100ms
6. âœ… Add Redis caching
7. âœ… Implement Django direct tools

---

## Monitoring & Validation

### Add Timing Metrics
**In Webhook Response:**
```javascript
// Add to Conversation Agent - Before Response
$json.metrics = {
  tool_calls_ms: {{ calculateToolTime() }},
  llm_first_token_ms: {{ calculateTTFT() }},
  total_response_ms: {{ $now - $json.request_start }}
}
```

### Target Metrics
- Tool execution: <100ms
- First token (TTFT): <500ms
- Full response: <1500ms
- Streaming chunks: Every 50-100ms

---

## Testing Script

```bash
# Test current latency
curl -X POST https://n8n-service-z1ur.onrender.com/webhook/besmart/voice/agent/ \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…",
    "session_id": "perf-test-1"
  }' \
  -w "\nTime: %{time_total}s\n"

# Expected results:
# Before: 2.5s - 3.5s
# After Phase 1: 1.5s - 2.0s
# After Phase 2: 1.2s - 1.5s
```

---

## Cost Impact

**Current (per 1000 requests):**
- GPT-4.1-mini: $0.15 input + $0.60 output = $0.75

**Optimized (gpt-4o-mini):**
- GPT-4o-mini: $0.15 input + $0.60 output = $0.75 (same cost, faster)

**If switching to GPT-3.5-turbo:**
- GPT-3.5: $0.05 input + $0.15 output = $0.20 (74% cost reduction)

---

## Summary

| Optimization | Latency Gain | Effort | Priority |
|-------------|--------------|--------|----------|
| Remove MCP loop | -300ms | 5 min | ðŸ”´ Critical |
| topK=1 | -100ms | 1 min | ðŸ”´ Critical |
| Model switch | -200ms | 2 min | ðŸŸ¡ High |
| Memory limit | -50ms | 2 min | ðŸŸ¡ High |
| Django direct | -300ms | 4 hours | ðŸŸ¢ Optional |

**Total Possible Gain: 950ms (60-70% reduction)**
